{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Battle of the Neighborhoods (Austin Food Tour): </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Introduction/Business Problem:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\">\tTravel and tourism is one of Texas’s biggest economic drivers, generating $80.2 billion in direct spending in 2018 [1]. People from all over the world flock to experience the unique culture and experience that the Lone Star State has to offer. Austin is one of the most tourist-friendly and famous cities in Texas. Austin hosts some of the most popular music festivals in the country like SXSW and the Austin City Limits Music Festival and is also home to the Circuit of The Americas. Because of the breadth of activities and experiences people can partake in, Austin draws in approximately 24.7 million domestic visitors each year [2].</p>\n",
    "\t<p style=\"font-size:18px\">Like many other cities, Austin is also home to some incredible restaurants. Tourists can indulge in unique cuisines all over the city and make their trip even more memorable. In this analysis, we are seeking to cluster the various neighborhoods in Austin by the types of restaurants they most frequently offer to understand what regions of Austin offer the most options of a specific type of cuisine. Then, we will be identifying the 5 most highly rated restaurants of the most popular cuisine for each cluster of neighborhoods in Austin to create a “must-go” restaurant list. </p>\n",
    "\t<p style=\"font-size:18px\">The stakeholders of this project are the city of Austin and its tourism board as this project will drive heavier tourist traffic around different areas of Austin. Travel bloggers, tourists, and college students will also benefit from this project as they can go to restaurants that are well-liked and experience cuisines that a particular neighborhood is known for. Restaurant owners have a great stake in this project as well because their restaurants may experience heavy foot-traffic because they were represented on this list. Restaurant owners who don’t rank as well may also use this listing and work to improve their rating.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\"> The data required for this project will be collected via web-scraping and by using Foursquare’s API. Web-scraping will be used to create a pandas DataFrame of all of the neighborhoods available in Central Austin. We will then get the Latitude and Longitude values of the neighborhoods using the Nominatim package in python and add those values to our DataFrame. The final data we will be using is location data from Foursquare. We will be querying Foursquare’s API for data on what specific restaurants exist within Austin’s neighborhoods and how well they are rated. This information will be used for determining which cuisines are most popular in which neighborhoods and will allow us to cluster them according to similarity. We will then be able to complete our list of the 5 most highly rated restaurants in each cluster of neighborhoods.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Analysis</h3>\n",
    "<p style=\"font-size:18px\">\n",
    "To begin our analysis, we will be scraping Wikipedia for a list of all Neighborhoods in Central Austin. We will do this using the requests and BeautifulSoup libraries and then transport the information to a pandas dataframe. To do this, we are going to be importing all of the libraries we will need.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from beautifulsoup4) (1.9.5)\n",
      "Requirement already satisfied: lxml in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (4.4.2)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests) (2019.9.11)\n",
      "Requirement already satisfied: geocoder in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (1.38.1)\n",
      "Requirement already satisfied: ratelim in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geocoder) (0.1.6)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geocoder) (2.22.0)\n",
      "Requirement already satisfied: future in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geocoder) (0.18.2)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geocoder) (1.13.0)\n",
      "Requirement already satisfied: click in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geocoder) (7.0)\n",
      "Requirement already satisfied: decorator in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from ratelim->geocoder) (4.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->geocoder) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->geocoder) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->geocoder) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->geocoder) (2019.9.11)\n",
      "Requirement already satisfied: folium in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: branca in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from folium) (0.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from folium) (2.10.3)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from folium) (2.22.0)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from folium) (1.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from jinja2->folium) (1.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->folium) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->folium) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->folium) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from requests->folium) (2019.9.11)\n",
      "Requirement already satisfied: geopy in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (1.20.0)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from geopy) (1.50)\n",
      "Packages fully loaded and installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install requests\n",
    "!pip install geocoder\n",
    "!pip install folium\n",
    "!pip install geopy\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geocoder\n",
    "import json \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "import numpy as np\n",
    "import geopy\n",
    "\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from geopy.geocoders import Nominatim \n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print('Packages fully loaded and installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\">Then, we will begin our web-scraping by defining the URL we will be using data from. We will go ahead and extract the data we need using the steps outlined below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bryker Woods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caswell Heights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Downtown Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eastwoods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hancock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Heritage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hyde Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Judges' Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lower Waller Creek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>North University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Oakmont Heights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Old Enfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Old Pecan Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Old West Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Original Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Original West University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pemberton Heights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ridgelea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ridgetop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rosedale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shoal Crest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>West Downtown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0               Neighborhood\n",
       "1               Bryker Woods\n",
       "2            Caswell Heights\n",
       "3            Downtown Austin\n",
       "4                  Eastwoods\n",
       "5                    Hancock\n",
       "6                   Heritage\n",
       "7                  Hyde Park\n",
       "8               Judges' Hill\n",
       "9         Lower Waller Creek\n",
       "10          North University\n",
       "11           Oakmont Heights\n",
       "12               Old Enfield\n",
       "13          Old Pecan Street\n",
       "14           Old West Austin\n",
       "15           Original Austin\n",
       "16  Original West University\n",
       "17         Pemberton Heights\n",
       "18                  Ridgelea\n",
       "19                  Ridgetop\n",
       "20                  Rosedale\n",
       "21               Shoal Crest\n",
       "22             West Downtown"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define URL source\n",
    "source = requests.get('https://en.wikipedia.org/wiki/List_of_Austin_neighborhoods').text\n",
    "\n",
    "#create a Beautiful Soup object and define lxml as the parser\n",
    "soup= bsoup(source, 'lxml')\n",
    "\n",
    "#isolate the Austin Neighborhood names and add them to an empty list\n",
    "table= soup.find('table',class_ = 'wikitable')\n",
    "tabledata = table.tbody.text.split('\\n\\n')\n",
    "tabledata\n",
    "tableitems = []\n",
    "\n",
    "#append items to table data\n",
    "for rows in tabledata:\n",
    "    temp  = rows.split('\\n')[1:]\n",
    "    if (temp != []):\n",
    "        tableitems.append(temp[0])\n",
    "\n",
    "df=pd.DataFrame(tableitems)\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:] \n",
    "df.columns = new_header\n",
    "df.rename(columns={'Name': \"Neighborhood\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\">Next, use the Nominatim Package to check the Latitude and Longitude coordinates to the DataFrame. Since Wikipedia is editable by any user, we want to make sure that all of the neighborhoods we scraped have coordinates present in Nominatim and that it does not return 'None'.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to handle TimeOuts from Geocoder\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "locator = Nominatim(user_agent = \"austinagent\")\n",
    "def do_geocode(address):\n",
    "    try:\n",
    "        return locator.geocode(address)\n",
    "    except GeocoderTimedOut:\n",
    "        return do_geocode(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bryker Woods']: 30.305246, -97.7545846\n",
      "['Caswell Heights']: None\n",
      "['Downtown Austin']: 30.2680536, -97.7447642\n",
      "['Eastwoods']: 30.290562350000002, -97.73141774152433\n",
      "['Hancock']: 30.2958956, -97.7247678\n",
      "['Heritage']: 30.3457961, -97.6909909\n",
      "['Hyde Park']: 30.3044117, -97.7304485\n",
      "[\"Judges' Hill\"]: None\n",
      "['Lower Waller Creek']: None\n",
      "['North University']: 30.42100665, -97.84008580625134\n",
      "['Oakmont Heights']: 30.3119891, -97.7540697\n",
      "['Old Enfield']: 30.2848643, -97.7591064\n"
     ]
    }
   ],
   "source": [
    "neighborhoods = df.values.tolist()\n",
    "\n",
    "#check if each neighborhood is returning data from Nominatim\n",
    "for neighborhood in neighborhoods:\n",
    "    g = do_geocode('{}, Austin, Texas'.format(neighborhood))\n",
    "    if (g == None):\n",
    "        print(str(neighborhood) + ': None')\n",
    "    else:\n",
    "        print(str(neighborhood) + ': ' + str(g.latitude) + ', ' + str(g.longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\">As you can see, some neighborhoods in our DataFrame are returning 'None' when Nominatim tries to find the coordinates for these locations. After further investigation, we found that Nominatim does not have coordinates for these neighborhoods, as shown in the screenshot below. For the purposes of this exercise, we will be either dropping or renaming the rows which do not have coordinates in Nominatim. The Neighborhoods that do not have coordinates are \"Caswell Heights\", \"Judges' Hill\", \"Lower Waller Creek\", and \"Original West University\". Nominatim does not have data on a \"Lower Waller Creek\", but it does have data on \"Waller Creek\". For this reason, we will be renaming this neighborhood, but we will be dropping the rest.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace neighborhood\n",
    "df.replace(['Lower Waller Creek'],['Waller Creek'], inplace=True)\n",
    "\n",
    "#add coordinate items to list with new dataframe\n",
    "neighborhoods = df.values.tolist()\n",
    "latitude = []\n",
    "longitude = []\n",
    "for neighborhood in neighborhoods:\n",
    "    g = do_geocode('{}, Austin, Texas'.format(neighborhood))\n",
    "    if (g == None):\n",
    "        latitude.append('0')\n",
    "        longitude.append('0')\n",
    "    else:\n",
    "        latitude.append(g.latitude)\n",
    "        longitude.append(g.longitude)\n",
    "\n",
    "#add coordinates column to dataframe\n",
    "df['Latitude'] = latitude\n",
    "df['Longitude'] = longitude\n",
    "\n",
    "#remove neighborhoods with no coordinates\n",
    "df= df[df.Latitude!='0']\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.reset_index()\n",
    "df.drop(columns = ['index','level_0'], axis = 1, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px\">We have cleaned up our neighborhood DataFrame and removed all neighborhoods that do not return any coordinates from Nominatim. We are now ready to use the Foursquare API to get our restaurant data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sources</h2>\n",
    "<ol style=\"font-size:18px\"> <li>https://www.bizjournals.com/austin/news/2019/05/02/tourisms-economic-impact-in-texas-164-billion.html</li>\n",
    "<li>https://www.austintexas.org/travel-professionals/</li></ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
